---
title: "Interpretability of Riemannian tools used in Brain Computer Interfaces"
collection: publication
layout: single
permalink: /publication/2025-02-arxiv_WG
#excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2025-09-03
venue: 'IEEE Machine Learning For Signal Processing (MLSP)'
authors: 'TdS,  Tristan Venot, Marie-Constance Corsi, Florian Yger'
paperurl: 'http://thibaultdesurrel.github.io/files/MLSP_09.pdf'
HAL: 'https://hal.science/hal-05245110'
#arXiv: 'https://arxiv.org/abs/2502.01512'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---


Riemannian methods have established themselves as state-of-the-art approaches in Brain-Computer Interfaces (BCI) in terms of performance. However, their adoption by experimenters is often hindered by a lack of interpretability. In this work, we propose a set of tools designed to enhance practitioners’ understanding of the decisions made by Riemannian methods. Specifically, we develop techniques to quantify and visualize the influence of the different sensors on classification outcomes. Our approach includes a visualization tool for high-dimensional covariance matrices, a classifier-agnostic tool that focuses on the classification process, as well as methods that leverage the data’s topology to better characterize the role of each sensor. We demonstrate these tools on a specific dataset and provide Python code to facilitate their use by practitioners, thereby promoting the adoption of Riemannian methods in BCI.